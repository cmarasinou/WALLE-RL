{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.n_states = 12\n",
    "        self.h = 3\n",
    "        self.w = 4\n",
    "        self.states_locations = [(0,0), (0,1), (0,2), (0,3),\n",
    "                    (1,0), (1,1), (1,2),(1,3),\n",
    "                    (2,0), (2,1), (2,2),(2,3)]\n",
    "        self.states_index = np.array([i for i in range(0,self.n_states)]).reshape(self.h,self.w)\n",
    "        self.current_state= 0 #index of the current state\n",
    "\n",
    "        self.terminal_states = [7,11]\n",
    "        self.impossible_states = [5]\n",
    "        self.n_actions = 4\n",
    "        # in order E,W, N, S\n",
    "        self.actions = [(0,-1),(0,+1),(-1,0),(+1,0)]\n",
    "        self.action_symbols = np.array(['<','>','^','v'])\n",
    "        self.rewards = np.array([-0.04, -0.04, -0.04, -0.04,\n",
    "               -0.04, 0.0, -0.04,  -1.0,\n",
    "               -0.04, -0.04, -0.04,   1.0])\n",
    "        self.initial_reward = self.rewards[self.current_state]\n",
    "        # initialize transition model\n",
    "        self.transition_model = self.generate_transition_model()\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        # transition to new state\n",
    "        self.current_state = np.random.choice(self.n_states, \n",
    "                     p=self.transition_model[:,action,self.current_state])\n",
    "        if self.current_state in self.terminal_states:\n",
    "            end_state = self.current_state\n",
    "            reward = self.rewards[end_state]\n",
    "\n",
    "            print(\"Episode finished\")\n",
    "            #Reinitialize\n",
    "            self.__init__()\n",
    "            return end_state, reward, True # Last bool indicates that episode finished\n",
    "                \n",
    "        return self.current_state, self.rewards[self.current_state], False\n",
    "        \n",
    "    def out_of_bounds(self,state_location):\n",
    "        if state_location[0] in range(0,self.h) and state_location[1] in range(0,self.w):\n",
    "            if self.states_index[state_location] in self.impossible_states:\n",
    "                return True\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def generate_transition_model(self):\n",
    "        P = np.zeros((self.n_states,self.n_actions,self.n_states))\n",
    "\n",
    "        for s in range(0,self.n_states):\n",
    "            for a in range(0,self.n_actions):\n",
    "                if s in self.terminal_states or s in self.impossible_states:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                s_location = self.states_locations[s]\n",
    "\n",
    "                sp_location =  (s_location[0] + self.actions[a][0],s_location[1] + self.actions[a][1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.8\n",
    "                P[sp,a,s]+=prob\n",
    "\n",
    "                opposite_actions = 1-np.abs(self.actions[a])\n",
    "\n",
    "                sp_location =  (s_location[0] + opposite_actions[0], s_location[1] +opposite_actions[1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.1\n",
    "                P[sp,a,s]+=prob\n",
    "\n",
    "\n",
    "                sp_location =  (s_location[0] - opposite_actions[0], s_location[1] -opposite_actions[1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.1\n",
    "                P[sp,a,s]+=prob\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.n_states = 12\n",
    "        self.h = 3\n",
    "        self.w = 4\n",
    "        self.states_locations = [(0,0), (0,1), (0,2), (0,3),\n",
    "                    (1,0), (1,1), (1,2),(1,3),\n",
    "                    (2,0), (2,1), (2,2),(2,3)]\n",
    "        self.states_index = np.array([i for i in range(0,self.n_states)]).reshape(self.h,self.w)\n",
    "        self.n_actions = 4\n",
    "        # in order E,W, N, S\n",
    "        self.actions = [(0,-1),(0,+1),(-1,0),(+1,0)]\n",
    "        self.action_symbols = np.array(['<','>','^','v'])\n",
    "        \n",
    "        # Defining policy\n",
    "        policy_symbols = np.array([['v' ,'<', '<' ,'<'],\n",
    "                                   ['v', None ,'v', None],\n",
    "                                   ['>' ,'>', '>', None]])\n",
    "        self.policy = self.translate_policy(policy_symbols)\n",
    "        \n",
    "    def translate_policy(self,policy_symbols):\n",
    "        policy_symbols = policy_symbols.ravel()\n",
    "        policy = np.zeros_like(policy_symbols, dtype=int)\n",
    "        for i, symbol in enumerate(policy_symbols):\n",
    "            if symbol is None:\n",
    "                policy[i] = -1\n",
    "            else:\n",
    "                policy[i] = np.argmax(self.action_symbols==symbol)\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    def step(self, state):\n",
    "        action = self.policy[state]\n",
    "        print(\"Action: \",self.action_symbols[action])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_episode(utility, alpha=0.1, gamma=0.9):\n",
    "    agent = Agent()\n",
    "    env = Environment()\n",
    "    state = env.current_state\n",
    "    reward = env.initial_reward\n",
    "    episode_finished=False\n",
    "    print(utility.reshape(3,4))\n",
    "    while not episode_finished:\n",
    "        action = agent.step(state)\n",
    "        state_prime, reward_prime, episode_finished = env.step(action)\n",
    "        utility[state] = utility[state]+alpha*(reward_prime+gamma*utility[state_prime]-utility[state])\n",
    "        print(env.states_locations[state],\" to \", env.states_locations[state_prime])\n",
    "        print(utility.reshape(3,4))\n",
    "        \n",
    "        state, reward = state_prime, reward_prime\n",
    "        \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0124636  0.         0.         0.       ]\n",
      " [-0.0182956  0.         0.01103    0.       ]\n",
      " [-0.011038   0.01436    0.2503     0.       ]]\n",
      "Action:  v\n",
      "(0, 0)  to  (1, 0)\n",
      "[[-0.01686384  0.          0.          0.        ]\n",
      " [-0.0182956   0.          0.01103     0.        ]\n",
      " [-0.011038    0.01436     0.2503      0.        ]]\n",
      "Action:  v\n",
      "(1, 0)  to  (2, 0)\n",
      "[[-0.01686384  0.          0.          0.        ]\n",
      " [-0.02145946  0.          0.01103     0.        ]\n",
      " [-0.011038    0.01436     0.2503      0.        ]]\n",
      "Action:  >\n",
      "(2, 0)  to  (2, 1)\n",
      "[[-0.01686384  0.          0.          0.        ]\n",
      " [-0.02145946  0.          0.01103     0.        ]\n",
      " [-0.0126418   0.01436     0.2503      0.        ]]\n",
      "Action:  >\n",
      "(2, 1)  to  (2, 2)\n",
      "[[-0.01686384  0.          0.          0.        ]\n",
      " [-0.02145946  0.          0.01103     0.        ]\n",
      " [-0.0126418   0.031451    0.2503      0.        ]]\n",
      "Action:  >\n",
      "Episode finished\n",
      "(2, 2)  to  (2, 3)\n",
      "[[-0.01686384  0.          0.          0.        ]\n",
      " [-0.02145946  0.          0.01103     0.        ]\n",
      " [-0.0126418   0.031451    0.32527     0.        ]]\n"
     ]
    }
   ],
   "source": [
    "utility_i = np.zeros(12)\n",
    "for i in range(0,1):\n",
    "    df = pd.DataFrame([[0]+list(utility_i)],columns=['iter','(0, 0)',\n",
    "                                 '(0, 1)',\n",
    "                                 '(0, 2)',\n",
    "                                 '(0, 3)',\n",
    "                                 '(1, 0)',\n",
    "                                 'x',\n",
    "                                 '(1, 2)',\n",
    "                                 '-1',\n",
    "                                 '(2, 0)',\n",
    "                                 '(2, 1)',\n",
    "                                 '(2, 2)',\n",
    "                                 '1'])\n",
    "    \n",
    "    utility = TD_episode(utility)\n",
    "    df.loc[i+1] = [i+1]+list(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>(0, 0)</th>\n",
       "      <th>(0, 1)</th>\n",
       "      <th>(0, 2)</th>\n",
       "      <th>(0, 3)</th>\n",
       "      <th>(1, 0)</th>\n",
       "      <th>x</th>\n",
       "      <th>(1, 2)</th>\n",
       "      <th>-1</th>\n",
       "      <th>(2, 0)</th>\n",
       "      <th>(2, 1)</th>\n",
       "      <th>(2, 2)</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.016864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>0.32527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter    (0, 0)  (0, 1)  (0, 2)  (0, 3)    (1, 0)    x   (1, 2)   -1  \\\n",
       "0   0.0  0.000000     0.0     0.0     0.0  0.000000  0.0  0.00000  0.0   \n",
       "1   1.0 -0.016864     0.0     0.0     0.0 -0.021459  0.0  0.01103  0.0   \n",
       "\n",
       "     (2, 0)    (2, 1)   (2, 2)    1  \n",
       "0  0.000000  0.000000  0.00000  0.0  \n",
       "1 -0.012642  0.031451  0.32527  0.0  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
