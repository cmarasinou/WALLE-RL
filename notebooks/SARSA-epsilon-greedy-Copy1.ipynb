{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gridworld import Environment, Agent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(q, eligibility_trace, alpha, gamma, state_prime, state, reward_prime, action_prime, action):\n",
    "    delta = reward_prime + gamma*q[state_prime, action_prime]-q[state, action]\n",
    "    q = q + alpha*delta*eligibility_trace\n",
    "    return q\n",
    "    \n",
    "def update_eligibilty_trace(eligibility_trace, gamma,lambd, state, action):\n",
    "    eligibility_trace_prime = gamma*lambd*eligibility_trace\n",
    "    eligibility_trace_prime[state, action] =  gamma*lambd*eligibility_trace[state,action]+1\n",
    "    return eligibility_trace_prime\n",
    "\n",
    "def update_policy(q):\n",
    "    policy = q.argmax(axis = 1)\n",
    "    return policy\n",
    "\n",
    "def initialize():\n",
    "    q = np.zeros((12,4))\n",
    "    eligibility_trace = np.zeros((12,4))\n",
    "    agent = Agent()\n",
    "    env = Environment(random_initial_state=True)\n",
    "    state = env.current_state\n",
    "    reward = env.initial_reward\n",
    "    episode_finished=False\n",
    "    return agent, env, state, reward, episode_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(convergence_criterion = 10000, alpha=0.01, gamma=0.999, lambd = 0.0, epsilon = None):\n",
    "    n_iters = 0\n",
    "    same_policy_iter = 0\n",
    "    n_episodes = 0\n",
    "    q = np.zeros((12,4))\n",
    "    eligibility_trace = np.zeros((12,4))\n",
    "    agent, env, state, reward, episode_finished = initialize()\n",
    "    no_action_states = env.impossible_states + env.terminal_states\n",
    "    while same_policy_iter < convergence_criterion:\n",
    "        action = agent.step(state, epsilon = epsilon)\n",
    "        state_prime, reward_prime, episode_finished = env.step(action)\n",
    "        action_prime =  agent.step(state_prime) # Just observing the action, not applying it\n",
    "        eligibility_trace = update_eligibilty_trace(eligibility_trace, gamma,lambd, state, action)\n",
    "        q = update_q(q, eligibility_trace, alpha,gamma,state_prime, state, reward_prime, action_prime, action)\n",
    "        previous_policy = agent.policy\n",
    "        agent.policy = update_policy(q)\n",
    "        state, reward = state_prime, reward_prime\n",
    "        current_policy = agent.policy\n",
    "        \n",
    "        previous_policy = previous_policy.ravel()\n",
    "        previous_policy[no_action_states] = 0\n",
    "        current_policy = current_policy.ravel()\n",
    "        current_policy[no_action_states] = 0\n",
    "    \n",
    "        \n",
    "        if np.array_equal(previous_policy, agent.policy):\n",
    "            same_policy_iter += 1\n",
    "        else:\n",
    "            same_policy_iter = 0\n",
    "        n_iters += 1\n",
    "        if n_iters%10000 == 0:\n",
    "            print('Iteration {} ---- Current policy same for {} iterations'.format(n_iters, same_policy_iter))\n",
    "            agent.render_policy()\n",
    "        if episode_finished:\n",
    "            n_episodes += 1\n",
    "            agent, env, state, reward, episode_finished = initialize()\n",
    "    print('Final iteration number {}'.format(n_iters))\n",
    "    return agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 ---- Current policy same for 0 iterations\n",
      "[['v' '>' 'v' '^']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 20000 ---- Current policy same for 2 iterations\n",
      "[['v' '<' 'v' '^']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 30000 ---- Current policy same for 0 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 40000 ---- Current policy same for 6880 iterations\n",
      "[['v' '<' '<' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Final iteration number 43120\n"
     ]
    }
   ],
   "source": [
    "sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 ---- Current policy same for 0 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 20000 ---- Current policy same for 0 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 30000 ---- Current policy same for 783 iterations\n",
      "[['v' '<' '<' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 40000 ---- Current policy same for 5 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 50000 ---- Current policy same for 6444 iterations\n",
      "[['v' '<' '<' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Final iteration number 53556\n"
     ]
    }
   ],
   "source": [
    "sarsa(lambd = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10000 ---- Current policy same for 3 iterations\n",
      "[['v' '>' 'v' '^']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 20000 ---- Current policy same for 6 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_policy = sarsa(epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
