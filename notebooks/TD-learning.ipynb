{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    ''' Generates a 3x4 world\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        self.n_states = 12\n",
    "        self.h = 3 # world height\n",
    "        self.w = 4 #world width\n",
    "        \n",
    "        # define states\n",
    "        self.states_locations = [(0,0), (0,1), (0,2), (0,3),\n",
    "                    (1,0), (1,1), (1,2),(1,3),\n",
    "                    (2,0), (2,1), (2,2),(2,3)]\n",
    "        self.states_index = np.array([i for i in range(0,self.n_states)]).reshape(self.h,self.w)\n",
    "        self.current_state= 2 \n",
    "        self.terminal_states = [7,11]\n",
    "        self.impossible_states = [5]\n",
    "        \n",
    "        # define actions\n",
    "        self.n_actions = 4\n",
    "        # in order Left, Right, Up, Down\n",
    "        self.actions = [(0,-1),(0,+1),(-1,0),(+1,0)]\n",
    "        self.action_symbols = np.array(['<','>','^','v'])\n",
    "        \n",
    "        # define rewards\n",
    "        self.rewards = np.array([-0.04, -0.04, -0.04, -0.04,\n",
    "               -0.04, 0.0, -0.04,  -1.0,\n",
    "               -0.04, -0.04, -0.04,   1.0])\n",
    "        self.initial_reward = self.rewards[self.current_state]\n",
    "        \n",
    "        # define transition model\n",
    "        self.transition_model = self.generate_transition_model()\n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        '''Performs a transition given an action\n",
    "        Returns:\n",
    "            current_state (int)\n",
    "            reward (float)\n",
    "            Bool: If True episode finished and environment re-initialized\n",
    "        '''\n",
    "        # transition to new state\n",
    "        self.current_state = np.random.choice(self.n_states, \n",
    "                     p=self.transition_model[:,action,self.current_state])\n",
    "        if self.current_state in self.terminal_states:\n",
    "            end_state = self.current_state\n",
    "            reward = self.rewards[end_state]\n",
    "            #Reinitialize\n",
    "            self.__init__()\n",
    "            return end_state, reward, True # Last bool indicates that episode finished\n",
    "                \n",
    "        return self.current_state, self.rewards[self.current_state], False\n",
    "        \n",
    "    def out_of_bounds(self,state_location):\n",
    "        '''Determines whether given state out of bounds'''\n",
    "        if state_location[0] in range(0,self.h) and state_location[1] in range(0,self.w):\n",
    "            if self.states_index[state_location] in self.impossible_states:\n",
    "                return True\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def generate_transition_model(self):\n",
    "        '''Generates the transition model\n",
    "        Returns:\n",
    "            P (3d array, (n_states,n_actions,n_states)): Transtion model probabilites\n",
    "        '''\n",
    "        P = np.zeros((self.n_states,self.n_actions,self.n_states))\n",
    "\n",
    "        for s in range(0,self.n_states):\n",
    "            for a in range(0,self.n_actions):\n",
    "                if s in self.terminal_states or s in self.impossible_states:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                s_location = self.states_locations[s]\n",
    "\n",
    "                sp_location =  (s_location[0] + self.actions[a][0],s_location[1] + self.actions[a][1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.8\n",
    "                P[sp,a,s]+=prob\n",
    "\n",
    "                opposite_actions = 1-np.abs(self.actions[a])\n",
    "\n",
    "                sp_location =  (s_location[0] + opposite_actions[0], s_location[1] +opposite_actions[1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.1\n",
    "                P[sp,a,s]+=prob\n",
    "\n",
    "\n",
    "                sp_location =  (s_location[0] - opposite_actions[0], s_location[1] -opposite_actions[1])\n",
    "                if self.out_of_bounds(sp_location):\n",
    "                    sp_location = s_location\n",
    "                sp = self.states_index[sp_location]\n",
    "                prob = 0.1\n",
    "                P[sp,a,s]+=prob\n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    '''Creates an agent\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.n_states = 12\n",
    "        self.h = 3\n",
    "        self.w = 4\n",
    "        #define states\n",
    "        self.states_locations = [(0,0), (0,1), (0,2), (0,3),\n",
    "                    (1,0), (1,1), (1,2),(1,3),\n",
    "                    (2,0), (2,1), (2,2),(2,3)]\n",
    "        self.states_index = np.array([i for i in range(0,self.n_states)]).reshape(self.h,self.w)\n",
    "        #define actions\n",
    "        self.n_actions = 4\n",
    "        # in order Left, Right, Up, Down\n",
    "        self.actions = [(0,-1),(0,+1),(-1,0),(+1,0)]\n",
    "        self.action_symbols = np.array(['<','>','^','v'])\n",
    "        \n",
    "        # Defining policy\n",
    "        policy_symbols = np.array([['v' ,'<', '<' ,'<'],\n",
    "                                   ['v', None ,'v', None],\n",
    "                                   ['>' ,'>', '>', None]])\n",
    "        self.policy = self.translate_policy(policy_symbols)\n",
    "        \n",
    "    def translate_policy(self,policy_symbols):\n",
    "        '''Gets policy in symbols. Outputs policy as array of integers'''\n",
    "        policy_symbols = policy_symbols.ravel()\n",
    "        policy = np.zeros_like(policy_symbols, dtype=int)\n",
    "        for i, symbol in enumerate(policy_symbols):\n",
    "            if symbol is None:\n",
    "                policy[i] = -1\n",
    "            else:\n",
    "                policy[i] = np.argmax(self.action_symbols==symbol)\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    def step(self, state):\n",
    "        '''Agent decides an action given a policy\n",
    "        Args:\n",
    "            state (int)\n",
    "        Retunrs\n",
    "            action (int)\n",
    "        '''\n",
    "        action = self.policy[state]\n",
    "        #print(\"Action: \",self.action_symbols[action])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Temporal Difference Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_episode(utility, alpha=0.1, gamma=0.9):\n",
    "    ''' Runs the temporal difference algorithm for one episode\n",
    "    Args:\n",
    "        utility (1d array, (n_states))\n",
    "    Returns:\n",
    "        utility (1d array, (n_states)): resulting utility\n",
    "    '''\n",
    "    agent = Agent()\n",
    "    env = Environment()\n",
    "    state = env.current_state\n",
    "    reward = env.initial_reward\n",
    "    episode_finished=False\n",
    "    #print(utility.reshape(3,4))\n",
    "    while not episode_finished:\n",
    "        action = agent.step(state)\n",
    "        state_prime, reward_prime, episode_finished = env.step(action)\n",
    "        utility[state] = utility[state]+alpha*(reward_prime+gamma*utility[state_prime]-utility[state])\n",
    "        #print(env.states_locations[state],\" to \", env.states_locations[state_prime])\n",
    "        #print(utility.reshape(3,4))\n",
    "        \n",
    "        state, reward = state_prime, reward_prime\n",
    "        \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = np.zeros(12)\n",
    "df = pd.DataFrame([[0]+list(utility)],columns=['iter','(0, 0)',\n",
    "                             '(0, 1)',\n",
    "                             '(0, 2)',\n",
    "                             '(0, 3)',\n",
    "                             '(1, 0)',\n",
    "                             'x',\n",
    "                             '(1, 2)',\n",
    "                             '-1',\n",
    "                             '(2, 0)',\n",
    "                             '(2, 1)',\n",
    "                             '(2, 2)',\n",
    "                             '1'])\n",
    "for i in range(100):\n",
    "\n",
    "    \n",
    "    utility = TD_episode(utility)\n",
    "    df.loc[i+1] = [int(i+1)]+list(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 100)\n",
    "# style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    " \n",
    "# create a color palette\n",
    "palette = plt.get_cmap('tab20')\n",
    " \n",
    "# multiple line plot\n",
    "num=0\n",
    "for column in df.drop('iter', axis=1):\n",
    "    num+=1\n",
    "    plt.plot(df['iter'], df[column], marker='', color=palette(num), linewidth=1.5, alpha=0.9, label=column)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=4, ncol=2, fontsize=13)\n",
    " \n",
    "# Add titles\n",
    "plt.title(\"TD\", fontsize=15, fontweight=0, color='black')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.ylim(-1.2,1.2)\n",
    "plt.savefig(\"./td_graph.png\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
