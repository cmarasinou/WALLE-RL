{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gridworld import Environment, Agent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(q, eligibility_trace, alpha, gamma, state_prime, state, reward_prime, action):\n",
    "    delta = reward_prime + gamma*np.max(q[state_prime,:])-q[state, action]\n",
    "    q = q + alpha*delta*eligibility_trace\n",
    "    return q\n",
    "    \n",
    "def update_eligibilty_trace(q, eligibility_trace, gamma,lambd, state, action):\n",
    "    if q[state, action]== q[state,:].max():\n",
    "        greedy = 1\n",
    "    else:\n",
    "        greedy = 0\n",
    "    eligibility_trace_prime = greedy*gamma*lambd*eligibility_trace\n",
    "    eligibility_trace_prime[state, action] =  greedy*gamma*lambd*eligibility_trace[state,action]+1\n",
    "    return eligibility_trace_prime\n",
    "\n",
    "def update_policy(q):\n",
    "    policy = q.argmax(axis = 1)\n",
    "    return policy\n",
    "\n",
    "def initialize():\n",
    "    q = np.zeros((12,4))\n",
    "    eligibility_trace = np.zeros((12,4))\n",
    "    agent = Agent()\n",
    "    agent.policy = np.random.randint(0,4,(12,)) # A random policy to be the policy to be followed: policy \\mu\n",
    "    env = Environment(random_initial_state=True)\n",
    "    state = env.current_state\n",
    "    reward = env.initial_reward\n",
    "    episode_finished=False\n",
    "    return agent, env, state, reward, episode_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(convergence_criterion =20000, alpha=0.01, gamma=0.999, lambd = 0.0, epsilon = None):\n",
    "    n_iters = 0\n",
    "    same_policy_iter = 0\n",
    "    n_episodes = 0\n",
    "    q = np.zeros((12,4))\n",
    "    eligibility_trace = np.zeros((12,4))\n",
    "    agent, env, state, reward, episode_finished = initialize()\n",
    "    no_action_states = env.impossible_states + env.terminal_states\n",
    "    agent.policy = np.array([3, 0, 0, 1, 3, 0, 3, 0, 1, 1, 1, 0]) # A suboptimal policy\n",
    "    current_policy = agent.policy # Just to initialize it\n",
    "    print('The policy being used')\n",
    "    agent.render_policy()\n",
    "    while same_policy_iter < convergence_criterion:\n",
    "        action = agent.step(state, epsilon = epsilon)\n",
    "        state_prime, reward_prime, episode_finished = env.step(action)\n",
    "        eligibility_trace = update_eligibilty_trace(q, eligibility_trace, gamma,lambd, state, action)\n",
    "        q = update_q(q, eligibility_trace, alpha,gamma,state_prime, state, reward_prime, action)\n",
    "        previous_policy = current_policy\n",
    "        current_policy = update_policy(q)\n",
    "        state, reward = state_prime, reward_prime\n",
    "        previous_policy = previous_policy.ravel()\n",
    "        previous_policy[no_action_states] = 0\n",
    "        current_policy = current_policy.ravel()\n",
    "        current_policy[no_action_states] = 0\n",
    "        \n",
    "        if np.array_equal(previous_policy,current_policy):\n",
    "            same_policy_iter += 1\n",
    "        else:\n",
    "            same_policy_iter = 0\n",
    "        n_iters += 1\n",
    "        if n_iters%50000 == 0:\n",
    "            print('Iteration {} ---- Current policy same for {} iterations'.format(n_iters, same_policy_iter))\n",
    "            agent.render_policy(custom_policy = current_policy)\n",
    "        if episode_finished:\n",
    "            n_episodes += 1\n",
    "            agent, env, state, reward, episode_finished = initialize()\n",
    "    print('Final iteration number {}'.format(n_iters))\n",
    "    agent.render_policy(custom_policy = current_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy being used\n",
      "[['v' '<' '<' '>']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Iteration 50000 ---- Current policy same for 8324 iterations\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n",
      "Final iteration number 61676\n",
      "[['v' '<' 'v' '<']\n",
      " ['v' '*' 'v' '*']\n",
      " ['>' '>' '>' '*']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sarsa(epsilon=0.2, lambd = 0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
